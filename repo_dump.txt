--- PROJECT STRUCTURE ---
.npmrc
README.md
app/api/stream/route.ts
app/globals.css
app/layout.tsx
app/page.tsx
components/player/BackgroundAudio.tsx
components/player/GlassPlayer.tsx
components/player/PlayerControls.tsx
components/scene/AudioVisualizer.tsx
components/scene/SceneBackground.tsx
components/scene/StarField.tsx
components/ui/NeonButton.tsx
components/ui/TechText.tsx
data/tracks.ts
hooks/useAudioEngine.ts
hooks/useAudioListener.ts
hooks/useCrossfader.ts
next-env.d.ts
next.config.js
package.json
postcss.config.js
public/tracks.json
scripts/generate_dump.js
tailwind.config.ts
tsconfig.json


--- START FILE: .npmrc ---
legacy-peer-deps=true

--- END FILE: .npmrc ---

--- START FILE: README.md ---
# DJ SarasvatÄ« - Cosmic Synthwave Audio Event

![Cosmic Aesthetic](https://images.unsplash.com/photo-1614728263952-84ea256f9679?q=80&w=1200&auto=format&fit=crop)

> *"The Cosmic DJ. An immersive audio experience powered by the void."*

**DJ SarasvatÄ«** is a Next.js web application designed with a high-end "Synthwave / Retro-Futuristic" aesthetic. It features a reactive 3D deep space background, neon accents, and a glassmorphism-based player interface that streams curated tracks via YouTube.

## ðŸŒŒ Features

-   **Immersive 3D Background**: A "Warp Speed" starfield and reactive geometric visualizers built with `Three.js` and `@react-three/fiber`. (Optimized for performance).
-   **Audio Integration**: Powered by `react-youtube`, seamlessly integrated into a custom UI.
    -   **Auto-Play**: Music starts automatically upon entering the experience.
    -   **Intro Skipping**: Intelligently skips intros for verified tracks (e.g., Depeche Mode).
    -   **Continuous Playback**: Auto-advances to the next track in the playlist.
-   **Cinema Mode**: A dedicated "Expand" toggle to view music videos in full-screen overlay mode while maintaining audio continuity.
-   **Neon Aesthetics**: Custom Tailwind CSS configuration for glowing fuchsia and cyan accents against a deep void black.
-   **Glassmorphism UI**: High-performance backdrop blur and transparency effects for a premium "HUD" feel.
-   **Dynamic Time Tracking**: Real-time progress and duration display.

## ðŸ›  Tech Stack

-   **Framework**: [Next.js 14](https://nextjs.org/) (App Router)
-   **Styling**: [Tailwind CSS v3](https://tailwindcss.com/)
-   **3D Graphics**:
    -   [`@react-three/fiber`](https://docs.pmnd.rs/react-three-fiber): React renderer for Three.js.
    -   [`@react-three/drei`](https://github.com/pmndrs/drei): Useful helpers for R3F.
    -   *Note: Post-processing (Bloom/Noise) is currently disabled for maximum stability.*
-   **Audio/Video**: `react-youtube` wrapper for iframe API.
-   **Icons**: [Lucide React](https://lucide.dev/)
-   **Deployment**: Vercel

## ðŸš€ Getting Started

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/angpa/dj-sarasvati.git
    cd dj-sarasvati
    ```

2.  **Install dependencies**:
    ```bash
    npm install
    # Note: Use --legacy-peer-deps if you encounter version conflicts with R3F
    npm install --legacy-peer-deps
    ```

3.  **Run the development server**:
    ```bash
    npm run dev
    ```

4.  Open [http://localhost:3000](http://localhost:3000) with your browser to enter the void.

## ðŸŽ¨ Customization

Key style configurations can be found in `tailwind.config.ts`:

-   **Colors**: `cosmic-black`, `neon-fuchsia`, `electric-cyan`.
-   **Fonts**: `Outfit` (Heading) and `JetBrains Mono` (Tech details).

--- END FILE: README.md ---

--- START FILE: app/api/stream/route.ts ---
import { NextRequest, NextResponse } from 'next/server';
import ytdl from '@distube/ytdl-core';

export const dynamic = 'force-dynamic'; // Prevent caching of the route itself, though we might want to cache RESULTS

export async function GET(req: NextRequest) {
    const { searchParams } = new URL(req.url);
    const videoId = searchParams.get('videoId');

    if (!videoId) {
        return NextResponse.json({ error: 'Missing videoId' }, { status: 400 });
    }

    try {
        if (!ytdl.validateID(videoId)) {
            return NextResponse.json({ error: 'Invalid videoId' }, { status: 400 });
        }

        const info = await ytdl.getInfo(videoId);

        // Filter for audio only, prefer highest audio quality
        const format = ytdl.chooseFormat(info.formats, {
            quality: 'highestaudio',
            filter: 'audioonly'
        });

        if (!format || !format.url) {
            return NextResponse.json({ error: 'No suitable audio format found' }, { status: 404 });
        }

        // Return the direct URL and some metadata
        return NextResponse.json({
            url: format.url,
            mimeType: format.mimeType,
            approxDurationMs: format.approxDurationMs,
            title: info.videoDetails.title
        });

    } catch (error: any) {
        console.error("Stream extraction failed:", error);
        return NextResponse.json({
            error: 'Failed to extract stream',
            details: error.message
        }, { status: 500 });
    }
}

--- END FILE: app/api/stream/route.ts ---

--- START FILE: app/globals.css ---
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 255, 255, 255;
  --background-start-rgb: 0, 0, 0;
  --background-end-rgb: 3, 0, 8;
}

body {
  color: rgb(var(--foreground-rgb));
  background: black;
  min-height: 100vh;
  overflow-x: hidden;
}

@layer utilities {
  .text-shadow-glow {
    text-shadow: 0 0 10px rgba(255, 255, 255, 0.5), 0 0 20px rgba(240, 171, 252, 0.3);
  }
}

--- END FILE: app/globals.css ---

--- START FILE: app/layout.tsx ---
import type { Metadata } from "next";
import { Outfit, JetBrains_Mono } from "next/font/google";
import dynamic from "next/dynamic";
import "./globals.css";

const SceneBackground = dynamic(() => import("@/components/scene/SceneBackground"), {
    ssr: false,
});

const outfit = Outfit({
    subsets: ["latin"],
    variable: "--font-outfit",
    weight: ["200", "400", "700"],
});

const jetbrainsMono = JetBrains_Mono({
    subsets: ["latin"],
    variable: "--font-jetbrains-mono",
});

export const metadata: Metadata = {
    title: "DJ SarasvatÄ« | Cosmic Event",
    description: "Immersive Audio Experience",
};

export default function RootLayout({
    children,
}: Readonly<{
    children: React.ReactNode;
}>) {
    return (
        <html lang="en">
            <body className={`${outfit.variable} ${jetbrainsMono.variable} font-heading antialiased bg-black text-white selection:bg-neon-fuchsia/30`}>
                <SceneBackground />
                <div className="relative z-10 min-h-screen flex flex-col">
                    {children}
                    <div className="fixed bottom-2 right-4 z-50 pointer-events-none opacity-30 text-[10px] font-mono tracking-widest text-white mix-blend-difference">
                        v{process.env.NEXT_PUBLIC_APP_VERSION}
                    </div>
                </div>
            </body>
        </html>
    );
}

--- END FILE: app/layout.tsx ---

--- START FILE: app/page.tsx ---
"use client";

import { useState, useRef, useEffect } from "react";
import GlassPlayer from "@/components/player/GlassPlayer";
import AudioVisualizer from "@/components/scene/AudioVisualizer";
import { Canvas } from "@react-three/fiber";
import NeonButton from "@/components/ui/NeonButton";
import TechText from "@/components/ui/TechText";
import PlayerControls from "@/components/player/PlayerControls";
import { tracks } from "@/data/tracks";
import BackgroundAudio from "@/components/player/BackgroundAudio";
import { Maximize2, Minimize2 } from "lucide-react";
import clsx from "clsx";
import { useAudioEngine } from "@/hooks/useAudioEngine";

type Deck = 'A' | 'B';

export default function Home() {
    const [hasEntered, setHasEntered] = useState(false);
    const [activeDeck, setActiveDeck] = useState<Deck>('A');

    // Track indices
    const [trackIndexA, setTrackIndexA] = useState(0);
    const [trackIndexB, setTrackIndexB] = useState(1);

    // Engine Controls
    const {
        loadTrack, play, pause, setCrossfade,
        analyser, isReady, beat,
        deckA, deckB, crossfade: engineCrossfade
    } = useAudioEngine();

    // Local volume state (master)
    const [volume, setVolume] = useState(80);

    // UI State
    const [currentTime, setCurrentTime] = useState(0);
    const [duration, setDuration] = useState(0);
    const [isCinemaMode, setIsCinemaMode] = useState(false);

    // Initial Load & Track Sync
    useEffect(() => {
        if (isReady && hasEntered) {
            // Load initial tracks
            loadTrack('A', tracks[trackIndexA].videoId);
            loadTrack('B', tracks[trackIndexB].videoId);
        }
    }, [isReady, hasEntered]);

    // Track Loading Effects
    // When indices change, load the new track into the engine
    useEffect(() => {
        if (hasEntered && isReady) loadTrack('A', tracks[trackIndexA].videoId);
    }, [trackIndexA, isReady, hasEntered, loadTrack]);

    useEffect(() => {
        if (hasEntered && isReady) loadTrack('B', tracks[trackIndexB].videoId);
    }, [trackIndexB, isReady, hasEntered, loadTrack]);


    // Playback Sync
    // Determine effective playback state from Engine
    const isPlayingA = deckA.isPlaying;
    const isPlayingB = deckB.isPlaying;

    // Derived active track for Display info
    const currentTrack = activeDeck === 'A' ? tracks[trackIndexA] : tracks[trackIndexB];

    // Auto-Mix Logic (Replaces Audio Listener)
    // We check the ACTIVE deck's remaining time
    const activeDeckState = activeDeck === 'A' ? deckA : deckB;
    useEffect(() => {
        if (activeDeckState.isPlaying && activeDeckState.duration > 0) {
            const remaining = activeDeckState.duration - activeDeckState.currentTime;
            // Crossfade 8 seconds before end (Energy Mixing)
            if (remaining < 8 && remaining > 0.5) {
                // Trigger Next if not already doing so
                // Check if B is not playing
                const otherDeckStart = activeDeck === 'A' ? !isPlayingB : !isPlayingA;
                if (otherDeckStart) {
                    handleNext();
                }
            }
        }
    }, [activeDeckState.currentTime, activeDeck, isPlayingA, isPlayingB]);

    // Progress Sync for UI
    useEffect(() => {
        setCurrentTime(activeDeckState.currentTime);
        setDuration(activeDeckState.duration);
    }, [activeDeckState.currentTime, activeDeckState.duration]);


    const handleEnter = async () => {
        setHasEntered(true);
        // Play Deck A
        setTimeout(() => play('A'), 1000); // Small delay to ensuring loading
    };

    const togglePlay = () => {
        if (activeDeck === 'A') {
            if (isPlayingA) pause('A'); else play('A');
        } else {
            if (isPlayingB) pause('B'); else play('B');
        }
    };

    // Crossfader Animation Ref
    const fadeRequestRef = useRef<number>();
    const fadeStartTimeRef = useRef<number>(0);
    const fadeDuration = 5000;
    const fadeTargetRef = useRef<number>(0); // 0 or 1

    const animateFade = () => {
        const now = Date.now();
        const elapsed = now - fadeStartTimeRef.current;
        const progress = Math.min(elapsed / fadeDuration, 1);

        // Lerp
        const start = fadeTargetRef.current === 1 ? 0 : 1;
        const current = start + (fadeTargetRef.current - start) * progress;

        setCrossfade(current);

        if (progress < 1) {
            fadeRequestRef.current = requestAnimationFrame(animateFade);
        } else {
            // Fade complete
            // Stop the OTHER deck
            if (fadeTargetRef.current === 0) pause('B'); // Faded to A
            else pause('A'); // Faded to B
        }
    };

    const handleNext = () => {
        console.log("Triggering Mix...");

        // Determine Next Deck
        const nextDeck = activeDeck === 'A' ? 'B' : 'A';
        // Calculate next track index for the INACTIVE deck to be ready?
        // Actually, if we are mixing TO B, B should already be loaded with the Next Track from previous cycle?
        // In a simple A->B->A list:
        // 1. Start A[0], B[1]. Active A.
        // 2. Mix to B. Active B. B[1] plays. A stops.
        // 3. Prepare A with [2].

        const nextIdx = (activeDeck === 'A' ? trackIndexB : trackIndexA) + 1; // Actually logic is simpler:
        // Current implementation:
        // A active. B is "next".
        // Mix to B.
        // Once mixed, update A to be "next + 1".

        if (nextDeck === 'A') {
            // Transitioning TO A.
            // Ensure A is playing
            play('A');
            fadeTargetRef.current = 0; // Fade to 0 (A)

            // Queue next track for B
            const nextTrackForB = (trackIndexA + 1 + 1) % tracks.length;
            // Wait, A is playing track X. B was X+1.
            // We go to B.
            // Now A needs X+2.
            // So when we handleNext FROM B later...
        } else {
            // Transitioning TO B
            play('B');
            fadeTargetRef.current = 1; // Fade to 1 (B)
        }

        fadeStartTimeRef.current = Date.now();
        cancelAnimationFrame(fadeRequestRef.current!);
        animateFade();

        setActiveDeck(nextDeck);

        // Update the "Old" deck to the new standard after a delay
        setTimeout(() => {
            const nextTrackIndex = (Math.max(trackIndexA, trackIndexB) + 1) % tracks.length;
            if (nextDeck === 'A') setTrackIndexB(nextTrackIndex);
            else setTrackIndexA(nextTrackIndex);
        }, 6000); // Wait for fade to finish
    };

    const handlePrev = () => {
        // Simplified: Just cut to start of current or prev track
    };

    const handleSeek = (e: React.MouseEvent<HTMLDivElement>) => {
        // Seek logic for Engine?
        // engine.seek(deck, time)
    };

    const formatTime = (time: number) => {
        if (!time || isNaN(time)) return "00:00";
        const minutes = Math.floor(time / 60);
        const seconds = Math.floor(time % 60);
        return `${minutes.toString().padStart(2, "0")}:${seconds.toString().padStart(2, "0")}`;
    };

    const toggleCinemaMode = () => setIsCinemaMode(!isCinemaMode);

    if (!hasEntered) {
        return (
            <main className="flex min-h-screen flex-col items-center justify-center p-4">
                <div className="flex flex-col items-center gap-8 animate-in fade-in zoom-in duration-1000">
                    <div className="text-center space-y-2">
                        <TechText animate>System Ready</TechText>
                        <h1 className="text-6xl md:text-8xl font-thin tracking-tighter text-transparent bg-clip-text bg-gradient-to-r from-neon-fuchsia-glow via-white to-electric-cyan-bright">
                            SARASVATÄª
                        </h1>
                        <p className="text-electric-cyan/60 tracking-[0.5em] text-sm uppercase">. Native Audio Engine .</p>
                    </div>

                    <div className="flex flex-col items-center gap-2">
                        <NeonButton onClick={handleEnter} className="mt-8 text-xl px-10 py-4" glow>
                            INITIALIZE CORE
                        </NeonButton>
                    </div>
                </div>
            </main>
        );
    }

    return (
        <main className="flex min-h-screen flex-col items-center justify-center p-4 transition-opacity duration-1000">
            {/* Audio Visualizer Backend */}
            {/* Note: R3F Canvas handles the 3D scene. The audio is now driven by 'analyser' */}
            <div className="absolute inset-0 -z-10 opacity-30 pointer-events-none">
                <div className="w-full h-full">
                    <Canvas>
                        <ambientLight intensity={0.5} />
                        <AudioVisualizer analyser={analyser} />
                    </Canvas>
                </div>
            </div>

            <GlassPlayer>
                <div className="flex flex-col items-center text-center space-y-4 w-full">
                    {/* Video Container (MUTED VISUALS) */}
                    <div
                        className={clsx(
                            "rounded-xl bg-black/50 shadow-inner flex items-center justify-center mb-4 border border-white/5 relative overflow-hidden group transition-all duration-500 ease-in-out",
                            isCinemaMode ? "fixed inset-0 z-50 w-full h-full rounded-none border-none bg-black" : "w-80 h-80 md:w-96 md:h-96"
                        )}
                    >
                        {/* Deck A Visuals */}
                        <div className={clsx("absolute inset-0 transition-opacity duration-500", activeDeck === 'A' ? "opacity-100 z-10" : "opacity-0 z-0")}>
                            <BackgroundAudio
                                videoId={tracks[trackIndexA].videoId}
                                isPlaying={isPlayingA} // Sync video with audio state
                                volume={0} // Muted
                                muted={true} // Strict mute
                                beat={beat}
                                onEnded={() => { }} // Audio engine handles logic now
                                className="w-full h-full"
                            />
                        </div>

                        {/* Deck B Visuals */}
                        <div className={clsx("absolute inset-0 transition-opacity duration-500", activeDeck === 'B' ? "opacity-100 z-10" : "opacity-0 z-0")}>
                            <BackgroundAudio
                                videoId={tracks[trackIndexB].videoId}
                                isPlaying={isPlayingB}
                                volume={0}
                                muted={true}
                                beat={beat}
                                onEnded={() => { }}
                                className="w-full h-full"
                            />
                        </div>

                        {/* Cinema Toggle Button */}
                        <button
                            onClick={toggleCinemaMode}
                            className={clsx(
                                "absolute z-50 p-2 rounded-full bg-black/50 backdrop-blur-md text-white/70 hover:text-white hover:bg-white/10 transition-all border border-white/10",
                                isCinemaMode ? "top-8 right-8" : "top-2 right-2 opacity-0 group-hover:opacity-100"
                            )}
                        >
                            {isCinemaMode ? <Minimize2 size={24} /> : <Maximize2 size={20} />}
                        </button>
                    </div>

                    <div className="space-y-1">
                        <h2 className="text-3xl font-light tracking-wide text-white">{currentTrack.title}</h2>
                        <p className="text-electric-cyan font-mono text-sm tracking-widest">ARTIST: {currentTrack.artist}</p>
                    </div>

                    {/* Progress Bar (Visual Only for now) */}
                    <div className="w-full h-1 bg-white/10 rounded-full mt-6 relative overflow-visible">
                        <div
                            className="absolute top-0 left-0 h-full bg-gradient-to-r from-neon-fuchsia to-electric-cyan shadow-[0_0_10px_#d946ef] transition-all duration-1000 ease-linear pointer-events-none"
                            style={{ width: `${duration > 0 ? (currentTime / duration) * 100 : 0}%` }}
                        />
                    </div>

                    <div className="flex justify-between w-full text-xs font-mono text-white/50 mt-1">
                        <span>{formatTime(currentTime)}</span>
                        <span>{formatTime(duration)}</span>
                    </div>

                    <PlayerControls
                        isPlaying={activeDeck === 'A' ? isPlayingA : isPlayingB}
                        onPlayPause={togglePlay}
                        onNext={handleNext}
                        onPrev={handlePrev}
                    />
                </div>
            </GlassPlayer>
        </main>
    );
}

--- END FILE: app/page.tsx ---

--- START FILE: components/player/BackgroundAudio.tsx ---
"use client";

import React, { useEffect, useRef } from 'react';
import YouTube, { YouTubeEvent, YouTubePlayer } from 'react-youtube';
import clsx from 'clsx';

interface BackgroundAudioProps {
    videoId: string;
    isPlaying: boolean;
    volume: number;
    introSkip?: number;
    outroSkip?: number;
    seekTime?: number | null; // Timestamp to seek to
    onEnded: () => void;
    onProgress?: (current: number, duration: number) => void;
    className?: string;
    disableAutoSkip?: boolean;
    muted?: boolean;
    beat?: boolean;
}

export default function BackgroundAudio({
    videoId,
    isPlaying,
    volume,
    introSkip = 0,
    outroSkip = 0,
    seekTime,
    onEnded,
    onProgress,
    className,
    disableAutoSkip = false,
    muted = false,
    beat = false
}: BackgroundAudioProps) {
    const playerRef = useRef<YouTubePlayer | null>(null);

    // Sync Play/Pause state
    useEffect(() => {
        try {
            if (playerRef.current && playerRef.current.internalPlayer) {
                if (isPlaying) {
                    playerRef.current.playVideo();
                } else {
                    playerRef.current.pauseVideo();
                }
            }
        } catch (e) { console.warn("Player state sync failed", e); }
    }, [isPlaying]);

    // Sync Volume
    useEffect(() => {
        try {
            // Check if player is truly ready and internal player exists
            if (playerRef.current && playerRef.current.internalPlayer && typeof playerRef.current.setVolume === 'function') {
                if (muted) {
                    playerRef.current.mute();
                } else {
                    playerRef.current.unMute();
                    playerRef.current.setVolume(volume);
                }
            }
        } catch (e) {
            console.warn("Failed to set volume on player:", e);
        }
    }, [volume, muted]);

    // Sync Seek
    useEffect(() => {
        if (!playerRef.current || seekTime === null || seekTime === undefined) return;
        playerRef.current.seekTo(seekTime, true);
    }, [seekTime]);

    // Poll for progress & Check Outro Skip
    useEffect(() => {
        let interval: NodeJS.Timeout;

        if (isPlaying) {
            interval = setInterval(() => {
                if (playerRef.current && typeof playerRef.current.getCurrentTime === 'function') {
                    const current = playerRef.current.getCurrentTime();
                    const duration = playerRef.current.getDuration();

                    // Report progress
                    if (onProgress) {
                        onProgress(current, duration);
                    }

                    // Check for Outro Skip
                    // Should be at least halfway through to avoid skipping immediately on broken metadata
                    if (!disableAutoSkip && outroSkip > 0 && duration > 0 && current > duration / 2) {
                        if (duration - current <= outroSkip) {
                            console.log("Auto-skipping outro");
                            onEnded(); // Trigger next track
                        }
                    }
                }
            }, 1000);
        }

        return () => {
            if (interval) clearInterval(interval);
        };
    }, [isPlaying, onProgress, outroSkip, onEnded, disableAutoSkip]);

    const onReady = (event: YouTubeEvent) => {
        console.log("Player Ready"); // Debug
        playerRef.current = event.target;

        // Ensure player is unmuted and set volume
        if (typeof event.target.unMute === 'function') {
            if (muted) event.target.mute();
            else {
                event.target.unMute();
                event.target.setVolume(volume);
            }
        }

        if (isPlaying) {
            if (introSkip > 0) {
                event.target.seekTo(introSkip, true);
            }
            event.target.playVideo();
        }
    };

    const opts = {
        height: '100%',
        width: '100%',
        playerVars: {
            autoplay: isPlaying ? 1 : 0,
            controls: 0,
            disablekb: 1,
            fs: 0,
            iv_load_policy: 3,
            modestbranding: 1,
            rel: 0,
            start: introSkip,
        },
    };

    return (
        <div className={className}>
            <YouTube
                videoId={videoId}
                opts={opts}
                onReady={onReady}
                onEnd={onEnded}
                onError={(e: any) => {
                    console.error("YouTube Player Error:", e);
                    // Force skip on fatal errors to avoid stuck state
                    onEnded();
                }}
                className={clsx(
                    "w-full h-full object-cover transition-all duration-75 ease-out",
                    isPlaying && beat ? "scale-105 brightness-125 contrast-125" : "scale-100 brightness-100"
                )}
                iframeClassName="w-full h-full object-cover"
            />
        </div>
    );
}

--- END FILE: components/player/BackgroundAudio.tsx ---

--- START FILE: components/player/GlassPlayer.tsx ---
import { ReactNode } from 'react';
import { clsx, type ClassValue } from 'clsx';
import { twMerge } from 'tailwind-merge';

function cn(...inputs: ClassValue[]) {
    return twMerge(clsx(inputs));
}

interface GlassPlayerProps {
    children: ReactNode;
    className?: string;
}

export default function GlassPlayer({ children, className }: GlassPlayerProps) {
    return (
        <div
            className={cn(
                "relative w-full max-w-2xl p-8 rounded-[2rem]",
                "bg-black/30 backdrop-blur-xl",
                "border border-white/10",
                "shadow-[0_0_60px_rgba(0,0,0,0.8)]",
                "flex flex-col items-center gap-6",
                className
            )}
        >
            {/* Decorative top line */}
            <div className="absolute top-0 left-1/2 -translate-x-1/2 w-32 h-1 bg-gradient-to-r from-transparent via-neon-fuchsia to-transparent opacity-50" />

            {children}
        </div>
    );
}

--- END FILE: components/player/GlassPlayer.tsx ---

--- START FILE: components/player/PlayerControls.tsx ---
import NeonButton from '../ui/NeonButton';
import { Play, Pause, SkipForward, SkipBack } from 'lucide-react'; // Assuming lucide-react is available or use SVGs

export default function PlayerControls({
    isPlaying,
    onPlayPause,
    onNext,
    onPrev
}: {
    isPlaying: boolean;
    onPlayPause: () => void;
    onNext: () => void;
    onPrev: () => void;
}) {
    return (
        <div className="flex items-center gap-6 mt-4">
            <NeonButton onClick={onPrev} variant="secondary" className="px-4 py-2" aria-label="Previous Track">
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><polygon points="19 20 9 12 19 4 19 20"></polygon><line x1="5" y1="19" x2="5" y2="5"></line></svg>
            </NeonButton>

            <NeonButton onClick={onPlayPause} variant="primary" className="px-8 py-3 text-lg" aria-label={isPlaying ? "Pause" : "Play"}>
                {isPlaying ? (
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><rect x="6" y="4" width="4" height="16"></rect><rect x="14" y="4" width="4" height="16"></rect></svg>
                ) : (
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg>
                )}
            </NeonButton>

            <NeonButton onClick={onNext} variant="secondary" className="px-4 py-2" aria-label="Next Track">
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><polygon points="5 4 15 12 5 20 5 4"></polygon><line x1="19" y1="5" x2="19" y2="19"></line></svg>
            </NeonButton>
        </div>
    );
}

--- END FILE: components/player/PlayerControls.tsx ---

--- START FILE: components/scene/AudioVisualizer.tsx ---
"use client";

import { useRef, useMemo } from "react";
import { useFrame } from "@react-three/fiber";
import { Sphere, Box, Icosahedron } from "@react-three/drei";
import * as THREE from "three";
import { EffectComposer, Bloom, ChromaticAberration, Noise } from "@react-three/postprocessing";

// Helper to get average volume from analyser
const getAverageVolume = (analyser: AnalyserNode | null, dataArray: Uint8Array) => {
    if (!analyser || !dataArray) return 0;
    try {
        analyser.getByteFrequencyData(dataArray as any);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i];
        }
        return dataArray.length > 0 ? sum / dataArray.length : 0; // 0-255
    } catch (e) {
        return 0;
    }
};

export default function AudioVisualizer({ analyser }: { analyser: AnalyserNode | null }) {
    const meshRef = useRef<THREE.Mesh>(null);
    const ringsRef = useRef<THREE.Group>(null);

    // Data array for frequency analysis
    const dataArray = useMemo(() => {
        return new Uint8Array(analyser ? analyser.frequencyBinCount : 128);
    }, [analyser]);

    useFrame((state) => {
        const time = state.clock.getElapsedTime();

        // Calculate volume
        let volume = 0;
        if (analyser) {
            volume = getAverageVolume(analyser, dataArray); // 0-255
        }

        // Normalized volume 0-1 (approx)
        const normVol = volume / 100;

        if (meshRef.current) {
            // Pulse based on real volume
            const scale = 2 + normVol * 1.5;
            meshRef.current.scale.setScalar(scale);
            meshRef.current.rotation.x = time * 0.5;
            meshRef.current.rotation.y = time * 0.3;
            (meshRef.current.material as THREE.MeshStandardMaterial).emissiveIntensity = 2 + normVol * 4;
        }

        if (ringsRef.current) {
            ringsRef.current.rotation.z = time * 0.1;
            ringsRef.current.children.forEach((child, i) => {
                const ring = child as THREE.Mesh;
                // Add wave effect to rings
                // const z = Math.sin(time * 2 + i) * normVol * 2;
                // ring.position.z = z;

                // Distort rings based on volume
                ring.scale.x = 1 + Math.sin(time * 2 + i) * 0.1 + (normVol * 0.5);
                ring.scale.z = 1 + Math.cos(time * 2 + i) * 0.1 + (normVol * 0.5);

                // Color shift based on intensity
                (ring.material as THREE.MeshStandardMaterial).emissiveIntensity = 0.5 + normVol * 2;
            });
        }
    });

    return (
        <>
            <group>
                <Sphere ref={meshRef} args={[1, 32, 32]}>
                    <meshStandardMaterial
                        color="#000000"
                        emissive="#d946ef"
                        emissiveIntensity={2}
                        roughness={0.1}
                        metalness={1}
                    />
                </Sphere>

                <group ref={ringsRef}>
                    {[...Array(3)].map((_, i) => (
                        <Box key={i} args={[3.5 + i * 1.5, 0.1, 3.5 + i * 1.5]} rotation={[0.5, 0, 0]}>
                            <meshStandardMaterial
                                color="#ff00ff"
                                emissive="#00ffff"
                                emissiveIntensity={0.5}
                                transparent
                                opacity={0.3}
                            />
                        </Box>
                    ))}
                </group>

                {/* Particles */}
                <FloatingCrystals analyser={analyser} dataArray={dataArray} />
            </group>

            {/* EffectComposer disabled for performance optimization to prevent Context Lost
            <EffectComposer>
                <Bloom luminanceThreshold={0.5} luminanceSmoothing={0.9} height={300} intensity={2.5} />
                <Noise opacity={0.05} />
                <ChromaticAberration offset={[0.002, 0.002] as any} />
            </EffectComposer>
            */}
        </>
    );
}

function FloatingCrystals({ analyser, dataArray }: { analyser: AnalyserNode | null, dataArray: Uint8Array }) {
    const groupRef = useRef<THREE.Group>(null);

    useFrame((state) => {
        const time = state.clock.getElapsedTime();
        let volume = 0;
        if (analyser) {
            // We reuse the dataArray from parent but it might be updated already? 
            // Better to re-calculate or accept that it's shared buffer.
            // For simplicity, re-calculate here is fine as it's cheap sum loop.
            // Actually, we can just pass the volume if we calculated it in parent, but passing props is cheaper than context.
            try {
                if (dataArray && dataArray.length > 0) {
                    analyser.getByteFrequencyData(dataArray as any);
                    let sum = 0;
                    for (let i = 0; i < dataArray.length; i++) {
                        sum += dataArray[i];
                    }
                    volume = (sum / dataArray.length) / 100;
                }
            } catch (e) { }
        }

        if (groupRef.current) {
            groupRef.current.rotation.y = time * 0.1;
            groupRef.current.children.forEach((child, i) => {
                const radius = 8;
                const angle = (i / 6) * Math.PI * 2;
                const yBase = Math.sin(angle * 2) * 2;

                const y = yBase + Math.sin(time + i * 10) * (1 + volume * 2);
                child.position.y = y;

                (child as THREE.Mesh).rotation.x = time * 0.5 + i;
                (child as THREE.Mesh).rotation.y = time * 0.3;
            });
        }
    });

    return (
        <group ref={groupRef}>
            {[...Array(6)].map((_, i) => {
                const radius = 8;
                const angle = (i / 6) * Math.PI * 2;
                const x = Math.cos(angle) * radius;
                const z = Math.sin(angle) * radius;

                return (
                    <Box key={i} args={[1, 1, 1]} position={[x, 0, z]}>
                        <meshStandardMaterial
                            color="#22d3ee"
                            emissive="#06b6d4"
                            emissiveIntensity={0.5}
                            transparent
                            opacity={0.8}
                        />
                    </Box>
                )
            })}
        </group>
    );
}

--- END FILE: components/scene/AudioVisualizer.tsx ---

--- START FILE: components/scene/SceneBackground.tsx ---
"use client";

import { Canvas } from "@react-three/fiber";
import { EffectComposer, Bloom, ChromaticAberration, Noise } from "@react-three/postprocessing";
import { BlendFunction } from 'postprocessing';
import StarField from "./StarField";
import AudioVisualizer from "./AudioVisualizer";

export default function SceneBackground() {
    return (
        <div className="fixed inset-0 z-0 bg-cosmic-black pointer-events-none">
            <Canvas
                camera={{ position: [0, 0, 15], fov: 75 }}
                gl={{ antialias: false, alpha: false }}
                dpr={[1, 2]} // Optimization for varying pixel ratios
            >
                <color attach="background" args={['#030008']} />

                {/* Ambient Light for base visibility */}
                <ambientLight intensity={0.5} />
                {/* Point Lights for accents */}
                <pointLight position={[10, 10, 10]} intensity={1} color="#f0abfc" />
                <pointLight position={[-10, -10, -10]} intensity={1} color="#22d3ee" />

                <StarField count={500} speed={1.5} />
                <AudioVisualizer analyser={null} />

                {/* <EffectComposer>
                    <Bloom
                        luminanceThreshold={0.2}
                        mipmapBlur
                        intensity={1.5}
                        radius={0.8}
                    />
                    <ChromaticAberration
                        blendFunction={BlendFunction.NORMAL} // Use blend mode if needed
                        offset={[0.002, 0.002]} // RGB shift
                    />
                    <Noise opacity={0.05} />
                </EffectComposer> */}
            </Canvas>
        </div>
    );
}

--- END FILE: components/scene/SceneBackground.tsx ---

--- START FILE: components/scene/StarField.tsx ---
"use client";

import { useRef, useMemo } from "react";
import { useFrame } from "@react-three/fiber";
import * as THREE from "three";

export default function StarField({ count = 400, speed = 1 }) {
  const pointsRef = useRef<THREE.Points>(null);

  // Generate random initial positions
  const particles = useMemo(() => {
    const positions = new Float32Array(count * 3);
    for (let i = 0; i < count; i++) {
      positions[i * 3] = (Math.random() - 0.5) * 800;
      positions[i * 3 + 1] = (Math.random() - 0.5) * 800;
      positions[i * 3 + 2] = (Math.random() - 0.5) * 800;
    }
    return positions;
  }, [count]);

  useFrame((state, delta) => {
    if (!pointsRef.current) return;

    const positions = pointsRef.current.geometry.attributes.position.array as Float32Array;

    for (let i = 0; i < count; i++) {
      // Move star towards camera (positive Z)
      positions[i * 3 + 2] += 20 * speed * delta * 5;

      // Reset when too close
      if (positions[i * 3 + 2] > 400) {
        positions[i * 3 + 2] = -400;
        positions[i * 3] = (Math.random() - 0.5) * 800;
        positions[i * 3 + 1] = (Math.random() - 0.5) * 800;
      }
    }

    pointsRef.current.geometry.attributes.position.needsUpdate = true;
  });

  return (
    <points ref={pointsRef}>
      <bufferGeometry>
        <bufferAttribute
          attach="attributes-position"
          count={particles.length / 3}
          array={particles}
          itemSize={3}
        />
      </bufferGeometry>
      <pointsMaterial size={2} color="#ffffff" sizeAttenuation transparent opacity={0.8} />
    </points>
  );
}

--- END FILE: components/scene/StarField.tsx ---

--- START FILE: components/ui/NeonButton.tsx ---
import { ButtonHTMLAttributes, ReactNode } from 'react';
import { clsx, type ClassValue } from 'clsx';
import { twMerge } from 'tailwind-merge';

function cn(...inputs: ClassValue[]) {
    return twMerge(clsx(inputs));
}

interface NeonButtonProps extends ButtonHTMLAttributes<HTMLButtonElement> {
    children: ReactNode;
    variant?: 'primary' | 'secondary' | 'danger';
    glow?: boolean;
}

export default function NeonButton({
    children,
    className,
    variant = 'primary',
    glow = true,
    ...props
}: NeonButtonProps) {
    const baseStyles = "relative px-6 py-2 rounded-full font-heading font-bold transition-all duration-300 ease-out border uppercase tracking-wider text-sm disabled:opacity-50 disabled:cursor-not-allowed";

    const variants = {
        primary: "bg-black/50 border-neon-fuchsia text-white hover:bg-neon-fuchsia hover:text-black hover:shadow-neon-pink-strong",
        secondary: "bg-black/50 border-electric-cyan text-white hover:bg-electric-cyan hover:text-black hover:shadow-neon-cyan",
        danger: "bg-black/50 border-red-500 text-white hover:bg-red-500 hover:text-black hover:shadow-[0_0_30px_rgba(239,68,68,0.5)]",
    };

    const glowStyles = glow ? (
        variant === 'primary' ? 'shadow-neon-pink' :
            variant === 'secondary' ? 'shadow-neon-cyan' : ''
    ) : '';

    return (
        <button
            className={cn(baseStyles, variants[variant], glowStyles, className)}
            {...props}
        >
            {children}
        </button>
    );
}

--- END FILE: components/ui/NeonButton.tsx ---

--- START FILE: components/ui/TechText.tsx ---
import { HTMLAttributes, ReactNode } from 'react';
import { clsx, type ClassValue } from 'clsx';
import { twMerge } from 'tailwind-merge';

function cn(...inputs: ClassValue[]) {
    return twMerge(clsx(inputs));
}

interface TechTextProps extends HTMLAttributes<HTMLSpanElement> {
    children: ReactNode;
    dimmed?: boolean;
    animate?: boolean;
}

export default function TechText({
    children,
    className,
    dimmed = false,
    animate = false,
    ...props
}: TechTextProps) {
    return (
        <span
            className={cn(
                "font-mono text-xs tracking-widest uppercase",
                dimmed ? "text-white/40" : "text-electric-cyan",
                animate && "animate-pulse",
                className
            )}
            {...props}
        >
            {children}
        </span>
    );
}

--- END FILE: components/ui/TechText.tsx ---

--- START FILE: data/tracks.ts ---
export interface Track {
    artist: string;
    title: string;
    videoId: string;
    introSkip?: number;
    outroSkip?: number; // Seconds to skip at the end (Fallback if AI is off)
}

export const tracks: Track[] = [
    {
        "artist": "Blondie",
        "title": "Call Me",
        "videoId": "StKVS0eI85I"
    },
    {
        "artist": "Madonna",
        "title": "Into the Groove",
        "videoId": "52iW3lcpK5M"
    },
    {
        "artist": "Prince and the Revolution",
        "title": "Kiss",
        "videoId": "H9tEvfIsDyo"
    },
    {
        "artist": "Depeche Mode",
        "title": "World in My Eyes",
        "videoId": "nhZdL4JlnxI",

    },
    {
        "artist": "Charly GarcÃ­a & Pedro Aznar",
        "title": "Hablando a Tu CorazÃ³n",
        "videoId": "Z7AERldCALc"
    }
];

--- END FILE: data/tracks.ts ---

--- START FILE: hooks/useAudioEngine.ts ---
import { useRef, useEffect, useState, useCallback } from 'react';

interface AudioEngineState {
    isReady: boolean;
    isPlaying: boolean;
    volume: number;
    crossfade: number; // 0 (Deck A) to 1 (Deck B)
    deckA: DeckState;
    deckB: DeckState;
}

interface DeckState {
    currentTime: number;
    duration: number;
    isPlaying: boolean;
    videoId: string | null;
}

export function useAudioEngine() {
    const audioContextRef = useRef<AudioContext | null>(null);
    const destinationRef = useRef<GainNode | null>(null); // Master Gain
    const analyserRef = useRef<AnalyserNode | null>(null);

    // Decks
    const deckA_Ref = useRef<HTMLAudioElement | null>(null);
    const deckB_Ref = useRef<HTMLAudioElement | null>(null);
    const gainA_Ref = useRef<GainNode | null>(null);
    const gainB_Ref = useRef<GainNode | null>(null);

    const [state, setState] = useState<AudioEngineState>({
        isReady: false,
        isPlaying: false,
        volume: 1,
        crossfade: 0,
        deckA: { currentTime: 0, duration: 0, isPlaying: false, videoId: null },
        deckB: { currentTime: 0, duration: 0, isPlaying: false, videoId: null },
    });

    // Initialize Audio Engine
    useEffect(() => {
        if (!typeof window) return;

        const ctx = new (window.AudioContext || (window as any).webkitAudioContext)();
        audioContextRef.current = ctx;

        // Master Chain
        const masterGain = ctx.createGain();
        masterGain.connect(ctx.destination);
        destinationRef.current = masterGain;

        const analyser = ctx.createAnalyser();
        analyser.fftSize = 256;
        masterGain.connect(analyser); // Analyze post-master volume
        analyserRef.current = analyser;

        // Create Decks (HTML Audio Elements are easier to stream than XHR+Buffer)
        const audioA = new Audio();
        audioA.crossOrigin = "anonymous";
        const sourceA = ctx.createMediaElementSource(audioA);
        const gainA = ctx.createGain();
        sourceA.connect(gainA).connect(masterGain);
        deckA_Ref.current = audioA;
        gainA_Ref.current = gainA;

        const audioB = new Audio();
        audioB.crossOrigin = "anonymous";
        const sourceB = ctx.createMediaElementSource(audioB);
        const gainB = ctx.createGain();
        sourceB.connect(gainB).connect(masterGain);
        deckB_Ref.current = audioB;
        gainB_Ref.current = gainB;

        // Set initial crossfade (Deck A active)
        gainA.gain.value = 1;
        gainB.gain.value = 0;

        // Event Listeners for State Updates
        const updateState = () => {
            setState(prev => ({
                ...prev,
                deckA: { ...prev.deckA, currentTime: audioA.currentTime, duration: audioA.duration || 0, isPlaying: !audioA.paused },
                deckB: { ...prev.deckB, currentTime: audioB.currentTime, duration: audioB.duration || 0, isPlaying: !audioB.paused },
            }));
        };

        audioA.addEventListener('timeupdate', updateState);
        audioB.addEventListener('timeupdate', updateState);
        audioA.addEventListener('play', () => { if (audioContextRef.current?.state === 'suspended') audioContextRef.current.resume(); updateState(); });
        audioB.addEventListener('play', () => { if (audioContextRef.current?.state === 'suspended') audioContextRef.current.resume(); updateState(); });
        audioA.addEventListener('pause', updateState);
        audioB.addEventListener('pause', updateState);

        setState(prev => ({ ...prev, isReady: true }));

        return () => {
            ctx.close();
            audioA.remove();
            audioB.remove();
        };
    }, []);

    // Methods
    const loadTrack = useCallback(async (deck: 'A' | 'B', videoId: string) => {
        try {
            const res = await fetch(`/api/stream?videoId=${videoId}`);
            if (!res.ok) throw new Error("Stream fetch failed");
            const data = await res.json();

            const audio = deck === 'A' ? deckA_Ref.current : deckB_Ref.current;
            if (audio) {
                audio.src = data.url;
                audio.load();
                setState(prev => ({
                    ...prev,
                    [deck === 'A' ? 'deckA' : 'deckB']: { ...prev[deck === 'A' ? 'deckA' : 'deckB'], videoId }
                }));
            }
        } catch (e) {
            console.error("Failed to load track:", e);
        }
    }, []);

    const play = useCallback(async (deck: 'A' | 'B') => {
        const audio = deck === 'A' ? deckA_Ref.current : deckB_Ref.current;
        if (audio) {
            // Resume context if needed
            if (audioContextRef.current?.state === 'suspended') {
                await audioContextRef.current.resume();
            }
            await audio.play();
        }
    }, []);

    const pause = useCallback((deck: 'A' | 'B') => {
        const audio = deck === 'A' ? deckA_Ref.current : deckB_Ref.current;
        if (audio) audio.pause();
    }, []);

    const setCrossfade = useCallback((value: number) => {
        // value 0 = Deck A, 1 = Deck B
        if (!gainA_Ref.current || !gainB_Ref.current) return;

        // Equal Power Crossfade curve (optional, using linear for now as requested)
        // Linear:
        const clamped = Math.max(0, Math.min(1, value));

        // Preventing rapid volume jumps
        const now = audioContextRef.current?.currentTime || 0;
        gainA_Ref.current.gain.setTargetAtTime(1 - clamped, now, 0.1);
        gainB_Ref.current.gain.setTargetAtTime(clamped, now, 0.1);

        setState(prev => ({ ...prev, crossfade: clamped }));
    }, []);

    // Beat Detection
    const [beat, setBeat] = useState(false);

    const analyzeBeat = useCallback(() => {
        if (!analyserRef.current) return;
        const bufferLength = analyserRef.current.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyserRef.current.getByteFrequencyData(dataArray);

        // Analyze sub-bass (first 10 bins, approx 0-200Hz depending on sample rate/fft)
        if (dataArray && dataArray.length > 0) {
            const lowEnd = dataArray.slice(0, 10);
            const avgLow = lowEnd.reduce((a, b) => a + b, 0) / lowEnd.length;

            // Dynamic threshold or fixed for now as per request > 210
            if (avgLow > 210) {
                setBeat(true);
                setTimeout(() => setBeat(false), 100);
            }
        }
        requestAnimationFrame(analyzeBeat);
    }, []);

    useEffect(() => {
        if (state.isReady) {
            analyzeBeat();
        }
    }, [state.isReady, analyzeBeat]);

    return {
        ...state,
        loadTrack,
        play,
        pause,
        setCrossfade,
        analyser: analyserRef.current,
        beat // Export beat state
    };
}

--- END FILE: hooks/useAudioEngine.ts ---

--- START FILE: hooks/useAudioListener.ts ---
import { useState, useEffect, useRef, useCallback } from 'react';

interface AudioListenerState {
    isListening: boolean;
    volume: number; // 0 to 1
    hasPermission: boolean;
    error: string | null;
}

interface AudioListenerControls {
    startListening: () => Promise<void>;
    stopListening: () => void;
}

export function useAudioListener(
    onSilenceDetected?: () => void,
    silenceThreshold: number = 0.02, // Very low volume threshold
    silenceDuration: number = 2000 // Time in ms below threshold to trigger
): AudioListenerState & AudioListenerControls {
    const [state, setState] = useState<AudioListenerState>({
        isListening: false,
        volume: 0,
        hasPermission: false,
        error: null,
    });

    const streamRef = useRef<MediaStream | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const analyserRef = useRef<AnalyserNode | null>(null);
    const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
    const rafIdRef = useRef<number | null>(null);
    const silenceStartRef = useRef<number | null>(null);

    const stopListening = useCallback(() => {
        if (rafIdRef.current) {
            cancelAnimationFrame(rafIdRef.current);
            rafIdRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
        }

        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        setState(prev => ({ ...prev, isListening: false, volume: 0 }));
    }, []);

    const analyzeAudio = useCallback(() => {
        if (!analyserRef.current || !state.isListening) return;

        const bufferLength = analyserRef.current.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyserRef.current.getByteFrequencyData(dataArray);

        // Calculate RMS (Volume)
        let sum = 0;
        for (let i = 0; i < bufferLength; i++) {
            sum += dataArray[i];
        }
        const average = sum / bufferLength;
        const normalizedVolume = average / 255; // Normalize to 0-1

        setState(prev => ({ ...prev, volume: normalizedVolume }));

        // Check for silence / fade out
        if (normalizedVolume < silenceThreshold) {
            if (silenceStartRef.current === null) {
                silenceStartRef.current = Date.now();
            } else {
                const elapsed = Date.now() - silenceStartRef.current;
                if (elapsed > silenceDuration) {
                    console.log("ðŸ”Š Silence/Fade-out detected! Triggering mix...");
                    silenceStartRef.current = null; // Reset
                    if (onSilenceDetected) onSilenceDetected();
                }
            }
        } else {
            silenceStartRef.current = null;
        }

        rafIdRef.current = requestAnimationFrame(analyzeAudio);
    }, [state.isListening, silenceThreshold, silenceDuration, onSilenceDetected]);

    // Re-trigger analysis loop when listening state changes
    useEffect(() => {
        if (state.isListening && !rafIdRef.current) {
            analyzeAudio();
        }
        return () => {
            if (rafIdRef.current) {
                cancelAnimationFrame(rafIdRef.current);
                rafIdRef.current = null;
            }
        };
    }, [state.isListening, analyzeAudio]);

    const startListening = useCallback(async () => {
        try {
            // Request screen capture with audio
            const stream = await navigator.mediaDevices.getDisplayMedia({
                video: true, // Required to get audio in most browsers
                audio: true,
                selfBrowserSurface: "include", // Explicitly allow capturing the current tab
                preferCurrentTab: true, // Hint to browser to prioritize current tab
            } as any); // Cast to any because TS might not have these experimental types yet

            // If user didn't share audio, we might get a track with no audio or just video
            const audioTracks = stream.getAudioTracks();
            if (audioTracks.length === 0) {
                throw new Error("No audio track found. Please ensure 'Share Tab Audio' is checked.");
            }

            streamRef.current = stream;

            // Handle stream ending (user stops sharing)
            stream.getVideoTracks()[0].onended = () => {
                stopListening();
            };

            const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            audioContextRef.current = audioContext;

            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            analyserRef.current = analyser;

            const source = audioContext.createMediaStreamSource(stream);
            sourceRef.current = source;
            source.connect(analyser);

            // Reverted: Connecting to destination caused "terrible echo".
            // It seems "unMute()" on the player is sufficient, or the browser isn't fully muting the tab.
            // If the user hears nothing, it's a Chrome policy issue, but Echo is worse.
            // source.connect(audioContext.destination);

            setState(prev => ({
                ...prev,
                isListening: true,
                hasPermission: true,
                error: null
            }));

        } catch (err: any) {
            console.error("Error starting audio listener:", err);
            setState(prev => ({
                ...prev,
                isListening: false,
                hasPermission: false,
                error: err.message || "Failed to access audio."
            }));
        }
    }, [stopListening]);

    return {
        ...state,
        startListening,
        stopListening
    };
}

--- END FILE: hooks/useAudioListener.ts ---

--- START FILE: hooks/useCrossfader.ts ---
import { useState, useCallback, useRef, useEffect } from 'react';

export type CrossfadeDirection = 'TO_A' | 'TO_B' | 'NONE';

interface UseCrossfaderProps {
    duration?: number; // Duration of fade in ms
    onFadeComplete?: () => void;
}

export function useCrossfader({ duration = 5000, onFadeComplete }: UseCrossfaderProps = {}) {
    const [ratio, setRatio] = useState(0); // 0 = Deck A, 1 = Deck B
    const [isFading, setIsFading] = useState(false);
    const [direction, setDirection] = useState<CrossfadeDirection>('NONE');

    const startTimeRef = useRef<number | null>(null);
    const rafRef = useRef<number | null>(null);
    const onCompleteRef = useRef(onFadeComplete);

    // Keep callback ref fresh
    useEffect(() => {
        onCompleteRef.current = onFadeComplete;
    }, [onFadeComplete]);

    const fadeTo = useCallback((target: 'A' | 'B') => {
        setIsFading(true);
        setDirection(target === 'A' ? 'TO_A' : 'TO_B');
        startTimeRef.current = performance.now();

        const startRatio = target === 'A' ? 1 : 0; // If fading to A, we start at B (1)
        // Ideally we should start from *current* ratio if interrupting, but for now simple swap.
        // Actually, if we are already at 0 (A), calling fadeTo A should do nothing? 
        // Let's assume we always cross from one to the other fully.

        const animate = (now: number) => {
            if (!startTimeRef.current) return;

            const elapsed = now - startTimeRef.current;
            const progress = Math.min(elapsed / duration, 1);

            // If Target A (0), we go from Current -> 0
            // If Target B (1), we go from Current -> 1
            // Simple linear fade:
            const newRatio = target === 'B' ? progress : (1 - progress);

            setRatio(newRatio);

            if (progress < 1) {
                rafRef.current = requestAnimationFrame(animate);
            } else {
                setIsFading(false);
                setDirection('NONE');
                startTimeRef.current = null;
                if (onCompleteRef.current) onCompleteRef.current();
            }
        };

        rafRef.current = requestAnimationFrame(animate);
    }, [duration]);

    // Force set (for checking or instant cuts)
    const setCrossfader = useCallback((val: number) => {
        setRatio(Math.max(0, Math.min(1, val)));
    }, []);

    return {
        ratio,       // 0 (A) to 1 (B)
        isFading,
        direction,
        fadeTo,
        setCrossfader
    };
}

--- END FILE: hooks/useCrossfader.ts ---

--- START FILE: next-env.d.ts ---
/// <reference types="next" />
/// <reference types="next/image-types/global" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/basic-features/typescript for more information.

--- END FILE: next-env.d.ts ---

--- START FILE: next.config.js ---
const pkg = require('./package.json');
/** @type {import('next').NextConfig} */
const nextConfig = {
    reactStrictMode: true,
    env: {
        NEXT_PUBLIC_APP_VERSION: pkg.version,
    },
    experimental: {
        serverComponentsExternalPackages: ['@distube/ytdl-core'],
    },
};

module.exports = nextConfig;

--- END FILE: next.config.js ---

--- START FILE: package.json ---
{
    "name": "dj-sarasvati",
    "private": true,
    "version": "2.0.1",
    "scripts": {
        "dev": "next dev",
        "build": "next build",
        "start": "next start"
    },
    "dependencies": {
        "@distube/ytdl-core": "^4.16.12",
        "@react-three/drei": "^10.7.7",
        "@react-three/fiber": "^8.18.0",
        "@react-three/postprocessing": "^3.0.4",
        "@types/three": "^0.182.0",
        "autoprefixer": "^10.4.23",
        "clsx": "^2.1.1",
        "lucide-react": "^0.561.0",
        "next": "14.1.0",
        "postcss": "^8.5.6",
        "react": "18.2.0",
        "react-dom": "18.2.0",
        "react-youtube": "^10.1.0",
        "tailwind-merge": "^3.4.0",
        "tailwindcss": "^3.4.19",
        "three": "^0.182.0"
    },
    "devDependencies": {
        "@types/node": "^20.0.0",
        "@types/react": "^18.0.0",
        "@types/react-dom": "^18.0.0",
        "typescript": "^5.0.0"
    }
}
--- END FILE: package.json ---

--- START FILE: postcss.config.js ---
module.exports = {
    plugins: {
        tailwindcss: {},
        autoprefixer: {},
    },
};

--- END FILE: postcss.config.js ---

--- START FILE: public/tracks.json ---
[
    {
        "artist": "Blondie",
        "title": "Call Me",
        "videoId": "StKVS0eI85I",
        "introSkip": 0
    },
    {
        "artist": "Madonna",
        "title": "Into the Groove",
        "videoId": "52iW3lcpK5M",
        "introSkip": 0
    },
    {
        "artist": "Prince",
        "title": "Kiss",
        "videoId": "3Y71iDvCYXA",
        "introSkip": 0
    },
    {
        "artist": "Depeche Mode",
        "title": "World in My Eyes",
        "videoId": "Qf0Qn_6Y8kA",
        "introSkip": 53
    },
    {
        "artist": "Charly GarcÃ­a & Pedro Aznar",
        "title": "Hablando a Tu CorazÃ³n",
        "videoId": "Z9nK0CqG9K0",
        "introSkip": 0
    }
]
--- END FILE: public/tracks.json ---

--- START FILE: scripts/generate_dump.js ---
#!/usr/bin/env node
const fs = require('fs');
const path = require('path');

const rootDir = path.join(__dirname, '..');
const outputFile = path.join(rootDir, 'repo_dump.txt');

const EXCLUDED_DIRS = [
    'node_modules',
    '.git',
    '.next',
    '.vercel',
    'dist',
    'build',
    'coverage',
];

const EXCLUDED_FILES = [
    'package-lock.json',
    '.DS_Store',
    'yarn.lock',
    'pnpm-lock.yaml',
    'repo_dump.txt',
    'repo_dump_FULL.txt',
    '.env',
    '.env.local',
    '.gitignore'
];

const INCLUDED_EXTENSIONS = [
    '.ts',
    '.tsx',
    '.js',
    '.jsx',
    '.json',
    '.css',
    '.scss',
    '.md',
    '.html'
];

function shouldIncludeFile(filePath) {
    const ext = path.extname(filePath);
    const filename = path.basename(filePath);

    if (EXCLUDED_FILES.includes(filename)) return false;

    // Exclude specific generated files
    if (filename.includes('player-script') && filename.endsWith('.js')) return false;

    // Size check (skip > 500KB)
    try {
        const stats = fs.statSync(filePath);
        if (stats.size > 500 * 1024) return false;
    } catch (e) {
        return false;
    }

    // Checking exact match for filename or if extension is in list
    // Note: some config files might be .js or .json, we generally want them, 
    // but we might want to exclude asset files like images.
    // The whitelist approach with INCLUDED_EXTENSIONS is safer for "text" dump.
    return INCLUDED_EXTENSIONS.includes(ext) ||
        filename === '.gitignore' ||
        filename === 'Dockerfile' ||
        filename === '.npmrc';
}

function scanDirectory(dir, fileList = []) {
    const files = fs.readdirSync(dir);

    for (const file of files) {
        const filePath = path.join(dir, file);
        const stat = fs.statSync(filePath);

        if (stat.isDirectory()) {
            if (!EXCLUDED_DIRS.includes(file)) {
                scanDirectory(filePath, fileList);
            }
        } else {
            if (shouldIncludeFile(filePath)) {
                fileList.push(filePath);
            }
        }
    }
    return fileList;
}

function generateDump() {
    console.log('Scanning directory...');
    const allFiles = scanDirectory(rootDir);
    console.log(`Found ${allFiles.length} files.`);

    let content = '';

    // Add file tree structure at the beginning
    content += '--- PROJECT STRUCTURE ---\n';
    // Simple tree visualization could be added here, but for now just list paths relative to root
    allFiles.forEach(f => {
        content += path.relative(rootDir, f) + '\n';
    });
    content += '\n\n';

    for (const filePath of allFiles) {
        const relativePath = path.relative(rootDir, filePath);
        console.log(`Processing ${relativePath}...`);

        try {
            const fileContent = fs.readFileSync(filePath, 'utf8');
            content += `--- START FILE: ${relativePath} ---\n`;
            content += fileContent;
            content += `\n--- END FILE: ${relativePath} ---\n\n`;
        } catch (err) {
            console.error(`Error reading ${relativePath}:`, err.message);
            content += `--- START FILE: ${relativePath} ---\n`;
            content += `[Error reading file: ${err.message}]\n`;
            content += `--- END FILE: ${relativePath} ---\n\n`;
        }
    }

    fs.writeFileSync(outputFile, content);
    console.log(`\nDump generated at: ${outputFile}`);
    console.log(`Total size: ${(fs.statSync(outputFile).size / 1024 / 1024).toFixed(2)} MB`);
}

generateDump();

--- END FILE: scripts/generate_dump.js ---

--- START FILE: tailwind.config.ts ---
import type { Config } from "tailwindcss";

const config: Config = {
    content: [
        "./pages/**/*.{js,ts,jsx,tsx,mdx}",
        "./components/**/*.{js,ts,jsx,tsx,mdx}",
        "./app/**/*.{js,ts,jsx,tsx,mdx}",
    ],
    theme: {
        extend: {
            colors: {
                "cosmic-black": "#000000",
                "void-purple": "#030008",
                "neon-fuchsia": {
                    DEFAULT: "#d946ef",
                    glow: "#f0abfc",
                },
                "electric-cyan": {
                    DEFAULT: "#06b6d4",
                    bright: "#22d3ee",
                },
            },
            fontFamily: {
                heading: ["var(--font-outfit)", "sans-serif"],
                mono: ["var(--font-jetbrains-mono)", "monospace"],
            },
            backgroundImage: {
                "gradient-cosmic": "linear-gradient(to right, #f0abfc, #ffffff, #22d3ee)",
            },
            boxShadow: {
                "neon-pink": "0 0 30px rgba(244, 114, 182, 0.3)",
                "neon-pink-strong": "0 0 50px rgba(244, 114, 182, 0.6)",
                "neon-cyan": "0 0 30px rgba(34, 211, 238, 0.3)",
            },
            animation: {
                pulse: "pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite",
                "spin-slow": "spin 20s linear infinite",
            },
        },
    },
    plugins: [],
};
export default config;

--- END FILE: tailwind.config.ts ---

--- START FILE: tsconfig.json ---
{
    "compilerOptions": {
        "target": "es5",
        "lib": [
            "dom",
            "dom.iterable",
            "esnext"
        ],
        "allowJs": true,
        "skipLibCheck": true,
        "strict": true,
        "noEmit": true,
        "esModuleInterop": true,
        "module": "esnext",
        "moduleResolution": "bundler",
        "resolveJsonModule": true,
        "isolatedModules": true,
        "jsx": "preserve",
        "incremental": true,
        "plugins": [
            {
                "name": "next"
            }
        ],
        "paths": {
            "@/*": [
                "./*"
            ]
        }
    },
    "include": [
        "next-env.d.ts",
        "**/*.ts",
        "**/*.tsx",
        ".next/types/**/*.ts"
    ],
    "exclude": [
        "node_modules"
    ]
}
--- END FILE: tsconfig.json ---

